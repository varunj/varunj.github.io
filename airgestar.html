<!DOCTYPE html>
<html lang="en-us">
<head>
<meta charset="UTF-8">
<title>AirGestAR</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="theme-color" content="#a90329">
<link rel="stylesheet" type="text/css" href="stylesheets_airgestar/normalize.css" media="screen">
<link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
<link rel="stylesheet" type="text/css" href="stylesheets_airgestar/stylesheet.css" media="screen">
<link rel="stylesheet" type="text/css" href="stylesheets_airgestar/github-light.css" media="screen">
<link rel="shortcut icon" href="favicon_airgestar.ico" />
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-101653083-1', 'auto');
  ga('send', 'pageview');

</script>
</head>
<body>
	<section class="page-header">
	  <h1 class="project-name">AirGestAR</h1>
	  <h2 class="project-tagline">Leveraging Deep Learning for Complex Hand Gestural Interaction with Frugal AR Devices</h2>
	<a href="#video1" class="btn">Demo Video 1</a>
	<a href="#video2" class="btn">Demo Video 2</a>
	<a href="https://github.com/varunj/AirGestAR" class="btn">Dataset and Codebase</a>
	</section>

    <section class="main-content">
		<h3><a id="welcome-to-airgest" class="anchor" href="#welcome-to-airgest" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Welcome to AirGestAR</h3>
		<p align="justify"> Hand gestures provide a natural and an intuitive way of user interaction in AR/VR applications. However, the most popular and commercially available devices such as the <a href="https://vr.google.com/cardboard/">Google Cardboard</a> and <a href="http://wearality.com/">Wearality</a> still employ only primitive modes of interaction such as the magnetic trigger, conductive lever and have limited user-input capability. The truly instinctual gestures work only with inordinately priced devices such as the <a href="https://www.microsoft.com/hololens">Hololens</a>, <a href="https://www.magicleap.com/">Magic Leap</a>, <a href="https://www.metavision.com/">Meta</a> which use proprietary hardware and are still not commercially available. </p>


		<h3><a id="the-idea" class="anchor" href="#the-idea" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>The Idea</h3>

		<p><img src="https://varunj.github.io/img_airgestar/fig2_architecture_nn.png"></p>

		<p align="justify">In this paper, we explore the possibility of leveraging deep learning for recognizing complex 3-dimensional marker-less gestures (Bloom, Click, Zoom-In, Zoom-Out) in real-time using monocular camera input from a single smartphone. This framework can be used with frugal smartphones to build powerful AR/VR systems for large scale deployments that work in real-time, eliminating the need for specialized hardware. We have created a hand gesture dataset to train LSTM networks for gesture classification and published the same online. We also demonstrate the performance of our proposed method in terms of classification accuracy and computational time. </p>


		<h3><a id="video1" class="anchor" href="#video1" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Demo Video: 1</h3>
		<video src="https://varunj.github.io/img_airgestar/demo_video_4_watermark.mp4" align='center' width="100%" height="100%" controls preload></video>
	    
	    	<h3><a id="video2" class="anchor" href="#video2" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Demo Video: 2</h3>
		<video src="https://varunj.github.io/img_airgestar/demo_video_0_watermark.mp4" align='center' width="100%" height="100%" controls preload></video>

    </section>

</body>
</html>
